# -*- coding: utf-8 -*-
"""Entrega 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QSsSyPf_S6DnBttgExiRL6Khc1kNbnyA

#PROCESAMIENTO
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder

# Cargar datos
link_train = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
link_test  = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"

# Nombres de columnas
columnas = [
    "age", "workclass", "fnlwgt", "education", "education_num",
    "marital_status", "occupation", "relationship", "race", "sex",
    "capital_gain", "capital_loss", "hours_per_week", "native_country", "income"
]

# Cargar train
train = pd.read_csv(link_train, header=None, names=columnas,
                    na_values=" ?", skipinitialspace=True)

# Cargar test
test = pd.read_csv(link_test, header=0, names=columnas,
                   na_values=" ?", skipinitialspace=True)

# Limpiar etiquetas
test["income"] = test["income"].str.replace(".", "", regex=False)

# Split 50/50 del test
test_1, test_2 = train_test_split(test, test_size=0.5, random_state=111)


# Separar features y target

# Train
X_train = train.drop("income", axis=1)
y_train = train["income"]

# Test_1
X_test1 = test_1.drop("income", axis=1)
y_test1 = test_1["income"]

# Test_2
X_test2 = test_2.drop("income", axis=1)
y_test2 = test_2["income"]

print("X_train:", X_train.shape, " y_train:", y_train.shape)
print("X_test1:", X_test1.shape, " y_test1:", y_test1.shape)
print("X_test2:", X_test2.shape, " y_test2:", y_test2.shape)

# Informaci贸n general y valores nulos
print("Train set info:")
print(train.info())

print("\nValores nulos por columna (train):")
print(train.isnull().sum())

print("\nTest set info:")
print(test.info())

print("\nValores nulos por columna (test):")
print(test.isnull().sum())

# Distribuci贸n de la variable objetivo
# Train
sns.countplot(data=train, x="income")
plt.title("Distribuci贸n de la variable objetivo (Train)")
plt.show()
print(train["income"].value_counts(normalize=True))

# Test
sns.countplot(data=test, x="income")
plt.title("Distribuci贸n de la variable objetivo (Test)")
plt.show()
print(test["income"].value_counts(normalize=True))

# Distribuci贸n de las variables categ贸ricas
categorical_cols = train.select_dtypes(include=["object"]).columns.drop("income")
print("Columnas categ贸ricas:", categorical_cols.tolist())

for col in categorical_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=train, y=col, order=train[col].value_counts().index)
    plt.title(f"Distribuci贸n de {col} (Train)")
    plt.show()

for col in categorical_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=test, y=col, order=train[col].value_counts().index)
    plt.title(f"Distribuci贸n de {col} (Test)")
    plt.show()

# Procesamiento de variables
# Definir columnas categ贸ricas y num茅ricas
cat_cols = [
    "workclass", "education", "marital_status",
    "occupation", "relationship", "race", "sex", "native_country"
]
num_cols = [
    "age", "fnlwgt", "education_num",
    "capital_gain", "capital_loss", "hours_per_week"
]

# Codificar la variable objetivo
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test1 = le.transform(y_test1)
y_test2 = le.transform(y_test2)

# Codificar variables categ贸ricas (OneHotEncoder)
encoder = OneHotEncoder(sparse_output=False)

X_train_cat = encoder.fit_transform(X_train[cat_cols])
X_test1_cat = encoder.transform(X_test1[cat_cols])
X_test2_cat = encoder.transform(X_test2[cat_cols])

# Pasar a DataFrames con nombres de columnas y mismo 铆ndice
cat_names = encoder.get_feature_names_out(cat_cols)
X_train_cat = pd.DataFrame(X_train_cat, columns=cat_names, index=X_train.index)
X_test1_cat = pd.DataFrame(X_test1_cat, columns=cat_names, index=X_test1.index)
X_test2_cat = pd.DataFrame(X_test2_cat, columns=cat_names, index=X_test2.index)

# Escalar variables num茅ricas
scaler = StandardScaler()

X_train_num = pd.DataFrame(
    scaler.fit_transform(X_train[num_cols]),
    columns=num_cols, index=X_train.index
)
X_test1_num = pd.DataFrame(
    scaler.transform(X_test1[num_cols]),
    columns=num_cols, index=X_test1.index
)
X_test2_num = pd.DataFrame(
    scaler.transform(X_test2[num_cols]),
    columns=num_cols, index=X_test2.index
)

# Unir categ贸ricas + num茅ricas
X_train_final = pd.concat([X_train_num, X_train_cat], axis=1)
X_test1_final = pd.concat([X_test1_num, X_test1_cat], axis=1)
X_test2_final = pd.concat([X_test2_num, X_test2_cat], axis=1)

# Revisar resultados finales
print("Train final:", X_train_final.shape)
print("Test1 final:", X_test1_final.shape)
print("Test2 final:", X_test2_final.shape)

# Mostrar primeras filas para verificar
print(X_train_final.head())
print(X_test1_final.head())
print(X_test2_final.head())

"""#MODELO BASELINE"""

# Baseline: Regresi贸n Log铆stica
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

# Entrenar el modelo
log_reg = LogisticRegression(max_iter=1000, random_state=111)
log_reg.fit(X_train_final, y_train)

# Funci贸n auxiliar para evaluar
def evaluar_modelo(modelo, X, y, dataset_name):
    y_pred = modelo.predict(X)  # predicciones finales
    y_prob = modelo.predict_proba(X)[:, 1]  # probabilidades para la clase positiva
    print(f"\nResultados en {dataset_name}:")
    print("Accuracy :", accuracy_score(y, y_pred))
    print("Precision:", precision_score(y, y_pred))
    print("Recall   :", recall_score(y, y_pred))
    print("F1-score :", f1_score(y, y_pred))
    print("AUC      :", roc_auc_score(y, y_prob))
    print("Matriz de confusi贸n:\n", confusion_matrix(y, y_pred))

# Evaluar en train, test1 y test2
evaluar_modelo(log_reg, X_train_final, y_train, "Train")
evaluar_modelo(log_reg, X_test1_final, y_test1, "Test1")
evaluar_modelo(log_reg, X_test2_final, y_test2, "Test2")

"""#MODELO DE REDES NEURONALES

##SIN DROPOUT NI EARLYSTOPPING
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import random
import numpy as np

# Fijar semilla para reproducibilidad
seed = 111
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

# Definir la arquitectura MLP con n煤mero variable de capas ocultas y neuronas
class MLP(nn.Module):
    def __init__(self, input_size, hidden_layers, hidden_neurons, output_size):
        super(MLP, self).__init__()
        layers = []
        layers.append(nn.Linear(input_size, hidden_neurons))
        layers.append(nn.ReLU())
        for _ in range(hidden_layers - 1):
            layers.append(nn.Linear(hidden_neurons, hidden_neurons))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(hidden_neurons, output_size))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# Funci贸n de entrenamiento que guarda las p茅rdidas en train y validaci贸n
def train_model(model, criterion, optimizer, train_loader, val_loader, epochs, device, exp_name="Exp"):
    train_losses, val_losses = [], []
    for epoch in range(epochs):
        # Entrenamiento
        model.train()
        running_loss = 0.0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y.unsqueeze(1).float())
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * batch_x.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_loss)

        # Validaci贸n
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y.unsqueeze(1).float())
                val_loss += loss.item() * batch_x.size(0)
        val_loss = val_loss / len(val_loader.dataset)
        val_losses.append(val_loss)

        # Imprimir resultados cada 10 茅pocas
        if (epoch + 1) % 10 == 0:
            print(f"[{exp_name}] poca {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}")

    return train_losses, val_losses

# Configuraci贸n del dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size = X_train_final.shape[1]
output_size = 1

# Conversi贸n de los datos a tensores
X_train_tensor = torch.tensor(X_train_final.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_test1_tensor = torch.tensor(X_test1_final.values, dtype=torch.float32)
y_test1_tensor = torch.tensor(y_test1, dtype=torch.long)

# Divisi贸n en train y validaci贸n
val_size = int(0.2 * len(X_train_tensor))
train_size = len(X_train_tensor) - val_size
train_dataset, val_dataset = torch.utils.data.random_split(
    TensorDataset(X_train_tensor, y_train_tensor), [train_size, val_size],
    generator=torch.Generator().manual_seed(seed)
)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

# Lista de experimentos con diferentes hiperpar谩metros
experiments = [
    {"hidden_layers": 2, "hidden_neurons": 128, "lr": 0.005, "epochs": 100},
    {"hidden_layers": 2, "hidden_neurons": 64, "lr": 0.001, "epochs": 100},
    {"hidden_layers": 3, "hidden_neurons": 256, "lr": 0.0002, "epochs": 100},
    {"hidden_layers": 4, "hidden_neurons": 128, "lr": 0.00005, "epochs": 100},
    {"hidden_layers": 2, "hidden_neurons": 128, "lr": 0.0005, "epochs": 100},
]

# Entrenar y graficar resultados
for i, exp in enumerate(experiments, 1):
    print(f"\n Entrenando Experimento {i}")
    model = MLP(input_size, exp["hidden_layers"], exp["hidden_neurons"], output_size).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=exp["lr"])

    train_losses, val_losses = train_model(
        model, criterion, optimizer, train_loader, val_loader,
        exp["epochs"], device, exp_name=f"Exp {i}"
    )

    # Gr谩fica de p茅rdidas
    plt.figure(figsize=(8,5))
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.title(f"P茅rdida vs pocas - Experimento {i}")
    plt.xlabel("pocas")
    plt.ylabel("P茅rdida")
    plt.legend()
    plt.show()

"""##CON DROPOUT Y EARLYSTOPPING"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np
import random

# Fijar semillas para que los resultados se repitan
def set_seed(seed=111):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(111)

# Red neuronal MLP con capas ocultas, ReLU y Dropout
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_layers, hidden_neurons, dropout_p=0.5):
        super(MLP, self).__init__()
        layers = []
        in_dim = input_dim

        for _ in range(hidden_layers):
            layers.append(nn.Linear(in_dim, hidden_neurons))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_p))
            in_dim = hidden_neurons

        layers.append(nn.Linear(in_dim, 1))
        layers.append(nn.Sigmoid())
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Entrenar modelo con EarlyStopping
def train_model(model, criterion, optimizer, train_loader, val_loader, epochs, device, patience=10):
    train_losses, val_losses = [], []
    best_val_loss = float("inf")
    counter = 0
    best_model_state = None

    for epoch in range(epochs):
        model.train()
        batch_losses = []
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device).float().view(-1, 1)
            optimizer.zero_grad()
            preds = model(xb)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            batch_losses.append(loss.item())

        train_loss = np.mean(batch_losses)

        # Validaci贸n
        model.eval()
        with torch.no_grad():
            val_batch_losses = []
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device).float().view(-1, 1)
                preds = model(xb)
                loss = criterion(preds, yb)
                val_batch_losses.append(loss.item())
            val_loss = np.mean(val_batch_losses)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        # Guardar mejor modelo
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict()
            counter = 0
        else:
            counter += 1
            if counter >= patience:
                print(f"癸 Early stopping en 茅poca {epoch+1}")
                model.load_state_dict(best_model_state)
                break

    return train_losses, val_losses, model

# Calcular m茅tricas de desempe帽o
def evaluate_model(model, loader, device):
    model.eval()
    y_true, y_pred, y_prob = [], [], []
    with torch.no_grad():
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device).cpu().numpy()
            probs = model(xb).cpu().numpy().flatten()
            preds = (probs > 0.5).astype(int)

            y_true.extend(yb)
            y_pred.extend(preds)
            y_prob.extend(probs)

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_prob)

    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1, "auc": auc}

# Lista de configuraciones a probar
experiments = [
    {"hidden_layers": 2, "hidden_neurons": 128, "dropout": 0.4, "lr": 0.005, "epochs": 100, "weight_decay": 1e-3},
    {"hidden_layers": 2, "hidden_neurons": 64, "dropout": 0.2, "lr": 0.001, "epochs": 100, "weight_decay": 5e-4},
    {"hidden_layers": 3, "hidden_neurons": 256, "dropout": 0.3, "lr": 0.0002, "epochs": 100, "weight_decay": 1e-2},
    {"hidden_layers": 4, "hidden_neurons": 128, "dropout": 0.5, "lr": 0.00005, "epochs": 100, "weight_decay": 1e-4},
    {"hidden_layers": 2, "hidden_neurons": 128, "dropout": 0.1, "lr": 0.0005, "epochs": 100, "weight_decay": 0},
]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim = next(iter(train_loader))[0].shape[1]
criterion = nn.BCELoss()

# Guardar resultados de todos los experimentos
results = []
for i, exp in enumerate(experiments, 1):
    print(f"\n===== Experimento {i} =====")
    model = MLP(input_dim, exp["hidden_layers"], exp["hidden_neurons"], exp["dropout"]).to(device)
    optimizer = optim.Adam(model.parameters(), lr=exp["lr"])

    train_losses, val_losses, trained_model = train_model(
        model, criterion, optimizer, train_loader, val_loader,
        exp["epochs"], device, patience=15
    )

    # Graficar p茅rdidas
    plt.figure()
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.title(f"P茅rdida vs pocas - Experimento {i}")
    plt.xlabel("pocas")
    plt.ylabel("P茅rdida")
    plt.legend()
    plt.show()

    # Evaluar en train, val y test
    train_metrics = evaluate_model(trained_model, train_loader, device)
    val_metrics = evaluate_model(trained_model, val_loader, device)
    test_metrics = evaluate_model(trained_model, test_loader, device)

    results.append({
        "exp": i,
        "config": exp,
        "train": train_metrics,
        "val": val_metrics,
        "test": test_metrics,
        "final_train_loss": train_losses[-1],
        "final_val_loss": val_losses[-1],
        "model": trained_model
    })

def composite_score(exp, alpha=0.5):
    val_auc = exp["val"]["auc"]
    gap = exp["final_train_loss"] - exp["final_val_loss"]
    gap = max(0, gap)
    return val_auc - alpha * gap

# Escoger mejor experimento seg煤n el score combinado
best_exp = max(results, key=lambda x: composite_score(x, alpha=0.1))

print("\n Mejor experimento:", best_exp["exp"])
print("Config:", best_exp["config"])
print("Train:", best_exp["train"])
print("Val:", best_exp["val"])
print("Test:", best_exp["test"])